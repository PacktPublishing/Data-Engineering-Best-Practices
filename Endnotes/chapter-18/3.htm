<html>

<head>
<meta http-equiv=Content-Type content="text/html; charset=windows-1252">
<meta name=Generator content="Microsoft Word 15 (filtered)">
<style>
<!--
 /* Font Definitions */
 @font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
@font-face
	{font-family:Aptos;}
 /* Style Definitions */
 a:link, span.MsoHyperlink
	{color:blue;
	text-decoration:underline;}
p.P-Regular, li.P-Regular, div.P-Regular
	{mso-style-name:"P - Regular";
	margin-top:6.0pt;
	margin-right:0in;
	margin-bottom:6.0pt;
	margin-left:0in;
	line-height:105%;
	font-size:11.0pt;
	font-family:"Aptos",sans-serif;}
p.P-Quote, li.P-Quote, div.P-Quote
	{mso-style-name:"P - Quote";
	margin:0in;
	line-height:115%;
	font-size:10.5pt;
	font-family:"Arial",sans-serif;
	font-style:italic;}
p.L-Numbers-1, li.L-Numbers-1, div.L-Numbers-1
	{mso-style-name:"L - Numbers-1";
	margin-top:0in;
	margin-right:0in;
	margin-bottom:4.0pt;
	margin-left:.75in;
	text-indent:-.25in;
	line-height:105%;
	font-size:11.0pt;
	font-family:"Aptos",sans-serif;}
span.P-URL
	{mso-style-name:"P - URL";
	font-family:"Calibri",sans-serif;
	color:blue;
	border:none windowtext 1.0pt;
	padding:0in;
	font-weight:bold;
	font-style:italic;
	text-decoration:underline;}
span.P-Endnote
	{mso-style-name:"P - Endnote";
	font-family:"Aptos",sans-serif;
	font-variant:normal !important;
	color:windowtext;
	text-transform:none;
	border:none windowtext 1.0pt;
	padding:0in;
	background:white;
	font-style:italic;
	text-decoration:none none;
	vertical-align:super;}
span.P-Keyword
	{mso-style-name:"P - Keyword";
	font-family:"Aptos",sans-serif;
	background:white;
	font-weight:bold;
	text-decoration:underline;}
span.P-Italics
	{mso-style-name:"P - Italics";
	font-family:"Arial",sans-serif;
	border:none windowtext 1.0pt;
	padding:0in;
	font-style:italic;}
.MsoChpDefault
	{font-size:10.0pt;}
.MsoPapDefault
	{margin-bottom:8.0pt;
	line-height:105%;}
@page WordSection1
	{size:8.5in 11.0in;
	margin:119.5pt 1.5in 131.75pt 1.5in;}
div.WordSection1
	{page:WordSection1;}
-->
</style>

</head>

<body lang=EN-US link=blue vlink=purple style='word-wrap:break-word'>

<div class=WordSection1>

<p class=P-Regular><span class=P-Endnote><span lang=EN-GB style='color:black;
border:none'>3</span></span><i><sup><span lang=EN-GB style='color:black;
background:white'><br>
</span></sup></i><span lang=EN-GB><a
href="https://towardsdatascience.com/something-of-thought-in-llm-prompting-an-overview-of-structured-llm-reasoning-70302752b390"><span
class=P-URL><span style='border:none'>&quot;Something-of-Thoughts in LLM
Prompting: An Overview of Structured LLM Reasoning&quot;</span></span></a> By
Yunzhe Wang, Towards Data Science; September 16, 2023; </span></p>

<p class=P-Regular><span lang=EN-GB>&nbsp;</span></p>

<p class=P-Quote><span lang=EN-GB>In the age of smartphones and smart homes,
imagine an </span><span class=P-Italics><span lang=EN-GB style='border:none;
font-style:normal'>AI</span></span><span lang=EN-GB> that doesn't merely follow
instructions, but actually thinks, grappling with complex logic just as we do.
Sounds like science fiction, doesn't it? However, if you've played around with </span><span
class=P-Italics><span lang=EN-GB style='border:none;font-style:normal'>ChatGPT</span></span><span
lang=EN-GB>, you've likely witnessed this astonishing capability firsthand. </span></p>

<p class=P-Quote><span lang=EN-GB>Even Hector Levesque, a renowned figure in AI
reasoning, was so astounded that he once commented to AI legend Geoffrey
Hinton: </span></p>

<p class=P-Quote><span lang=EN-GB>&nbsp;</span></p>

<p class=P-Quote><span class=P-URL><span lang=EN-GB style='border:none;
font-style:normal'><a href="https://www.youtube.com/watch?v=rGgGOccMEiY&amp;t=1598s">&quot;How
can such a stupid method (referring to neural networks) can deal with
reasoning?&quot;</a></span></span><span lang=EN-GB> (</span><span class=P-URL><span
lang=EN-GB style='border:none;font-style:normal'><a
href="https://www.youtube.com/watch?v=rGgGOccMEiY&amp;t=1598s">https://www.youtube.com/watch?v=rGgGOccMEiY&amp;t=1598s</a></span></span><span
lang=EN-GB>). </span></p>

<p class=P-Quote><span lang=EN-GB>&nbsp;</span></p>

<p class=P-Quote><span lang=EN-GB>While this story underscores the monumental
advances in AI, the true essence of these advancements is found in the
intricate dance of Large Language Models (</span><span class=P-Italics><span
lang=EN-GB style='border:none;font-style:normal'>LLMs</span></span><span
lang=EN-GB>) with reasoning. The entry point to this dance is </span><span
class=P-Italics><span lang=EN-GB style='border:none;font-style:normal'>Prompt
Engineering</span></span><span lang=EN-GB> - the art and science of optimizing
the textual input provided to </span><span class=P-Italics><span lang=EN-GB
style='border:none;font-style:normal'>LLMs</span></span><span lang=EN-GB> to
elicit desired outputs. At its core, it's about understanding the intricacies
of how language models like </span><span class=P-Italics><b><span lang=EN-GB
style='border:none;font-style:normal'>ChatGPT</span></b></span><span
lang=EN-GB>, </span><span class=P-Italics><b><span lang=EN-GB style='border:
none;font-style:normal'>Bard</span></b></span><span lang=EN-GB>, </span><span
class=P-Italics><b><span lang=EN-GB style='border:none;font-style:normal'>Claude</span></b></span><span
lang=EN-GB>, </span><span class=P-Italics><b><span lang=EN-GB style='border:
none;font-style:normal'>LLama</span></b></span><span lang=EN-GB>, and others
respond to different prompts, and then leveraging this knowledge to achieve
specific results. Think of </span><span class=P-Italics><span lang=EN-GB
style='border:none;font-style:normal'>LLMs</span></span><span lang=EN-GB> as
vast knowledge reservoirs. The way you phrase your question or statement (the
prompt) determines how you tap into that reservoir. Just as humans might offer
different answers based on how a question is posed, </span><span
class=P-Italics><span lang=EN-GB style='border:none;font-style:normal'>LLMs</span></span><span
lang=EN-GB> too can give varied responses based on the input. In this article,
you'll receive a concise overview of various prompt engineering frameworks
designed to enhance </span><span class=P-Italics><span lang=EN-GB
style='border:none;font-style:normal'>LLM</span></span><span lang=EN-GB>
reasoning, including:</span></p>

<p class=P-Quote><span lang=EN-GB>&nbsp;</span></p>

<p class=L-Numbers-1><span class=P-Italics><span style='border:none'>1.</span></span><span
class=P-Italics><span style='font-size:7.0pt;line-height:105%;font-family:"Times New Roman",serif;
border:none'>&nbsp;&nbsp;&nbsp; </span></span><span class=P-Italics><span
style='border:none'>Chain-of-Thought (</span></span><span class=P-Keyword><span
style='color:black'>CoT</span></span><span class=P-Italics><span
style='border:none'>)</span></span> See <a
href="https://arxiv.org/abs/2201.11903"><span class=P-URL><span
style='border:none'>https://arxiv.org/abs/2201.11903</span></span></a>;</p>

<p class=L-Numbers-1><span class=P-Italics><span style='border:none'>2.</span></span><span
class=P-Italics><span style='font-size:7.0pt;line-height:105%;font-family:"Times New Roman",serif;
border:none'>&nbsp;&nbsp;&nbsp; </span></span><span class=P-Italics><span
style='border:none'>Chain-of-Thought-Self-Consistency (</span></span><span
class=P-Keyword><span style='color:black'>CoT-SC</span></span><span
class=P-Italics><span style='border:none'>)</span></span> See <a
href="https://arxiv.org/abs/2203.11171"><span class=P-URL><span
style='border:none'>https://arxiv.org/abs/2203.11171</span></span></a>;</p>

<p class=L-Numbers-1><span class=P-Italics><span style='border:none'>3.</span></span><span
class=P-Italics><span style='font-size:7.0pt;line-height:105%;font-family:"Times New Roman",serif;
border:none'>&nbsp;&nbsp;&nbsp; </span></span><span class=P-Italics><span
style='border:none'>Tree-of-Thoughts (</span></span><span class=P-Keyword><span
style='color:black'>ToT</span></span><span class=P-Italics><span
style='border:none'>)</span></span> See <a
href="https://arxiv.org/abs/2305.10601"><span class=P-URL><span
style='border:none'>https://arxiv.org/abs/2305.10601</span></span></a>;</p>

<p class=L-Numbers-1><span class=P-Italics><span style='border:none'>4.</span></span><span
class=P-Italics><span style='font-size:7.0pt;line-height:105%;font-family:"Times New Roman",serif;
border:none'>&nbsp;&nbsp;&nbsp; </span></span><span class=P-Italics><span
style='border:none'>Graph-of-Thoughts (</span></span><span class=P-Keyword><span
style='color:black'>GoT</span></span><span class=P-Italics><span
style='border:none'>)</span></span> See <a
href="https://arxiv.org/abs/2308.09687"><span class=P-URL><span
style='border:none'>https://arxiv.org/abs/2308.09687</span></span></a>;</p>

<p class=L-Numbers-1><span class=P-Italics><span style='border:none'>5.</span></span><span
class=P-Italics><span style='font-size:7.0pt;line-height:105%;font-family:"Times New Roman",serif;
border:none'>&nbsp;&nbsp;&nbsp; </span></span><span class=P-Italics><span
style='border:none'>Algorithm-of-Thoughts (</span></span><span class=P-Keyword><span
style='color:black'>AoT</span></span><span class=P-Italics><span
style='border:none'>)</span></span> See <a
href="https://arxiv.org/abs/2308.10379"><span class=P-URL><span
style='border:none'>https://arxiv.org/abs/2308.10379</span></span></a>;</p>

<p class=L-Numbers-1><span class=P-Italics><span style='border:none'>6.</span></span><span
class=P-Italics><span style='font-size:7.0pt;line-height:105%;font-family:"Times New Roman",serif;
border:none'>&nbsp;&nbsp;&nbsp; </span></span><span class=P-Italics><span
style='border:none'>Skeleton-of-Thought (</span></span><span class=P-Keyword><span
style='color:black'>SoT</span></span><span class=P-Italics><span
style='border:none'>)</span></span> See <a
href="https://arxiv.org/abs/2307.15337"><span class=P-URL><span
style='border:none'>https://arxiv.org/abs/2307.15337</span></span></a>;</p>

<p class=L-Numbers-1><span class=P-Italics><span style='border:none'>7.</span></span><span
class=P-Italics><span style='font-size:7.0pt;line-height:105%;font-family:"Times New Roman",serif;
border:none'>&nbsp;&nbsp;&nbsp; </span></span><span class=P-Italics><span
style='border:none'>Program-of-Thoughts (</span></span><span class=P-Keyword><span
style='color:black'>PoT</span></span><span class=P-Italics><span
style='border:none'>)</span></span> See <a
href="https://arxiv.org/abs/2211.12588"><span class=P-URL><span
style='border:none'>https://arxiv.org/abs/2211.12588</span></span></a>;</p>

<p class=P-Regular><span lang=EN-GB>Refer to <a
href="https://towardsdatascience.com/something-of-thought-in-llm-prompting-an-overview-of-structured-llm-reasoning-70302752b390"><span
class=P-URL><span style='border:none'>https://towardsdatascience.com/something-of-thought-in-llm-prompting-an-overview-of-structured-llm-reasoning-70302752b390</span></span></a>.</span></p>

</div>

</body>

</html>
